{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating labels for audio and visuals at a single time. (to be ran first for lables)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "class Create_Labels():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.pattern = r'^frame_(\\d{6})_face_(\\d{2})\\.jpg$'\n",
    "\n",
    "    def load_segment(self, file_path):\n",
    "        return np.load(file_path)\n",
    "\n",
    "    def create_continuous_labels(self, total_samples, category):\n",
    "        label_value = 1 if category == \"PTSD\" else 0\n",
    "        return np.full(total_samples, label_value)\n",
    "\n",
    "    def save_continuous_labels(self, labels, video_folder):\n",
    "        output_filename = f\"{os.path.basename(video_folder)}_Continuous_Label.npy\"\n",
    "        output_path = os.path.join(video_folder, output_filename)\n",
    "        np.save(output_path, labels)\n",
    "\n",
    "    def get_category_from_path(self, file_path):\n",
    "        parts = file_path.split(os.sep)\n",
    "        return parts[-2]\n",
    "\n",
    "    def get_sorted_npy_segment_files(self, video_folder):\n",
    "        segment_files = [f for f in os.listdir(video_folder) if f.startswith(\"segment_\") and f.endswith(\".npy\")]\n",
    "        return sorted(segment_files, key=lambda x: int(re.findall(r'segment_(\\d+)\\.npy', x)[0]))\n",
    "\n",
    "\n",
    "    def get_sorted_visual_segment_folders(self, video_folder):\n",
    "        return sorted(\n",
    "            [f for f in os.listdir(video_folder) if f.startswith(\"segment_\") and os.path.isdir(os.path.join(video_folder, f))],\n",
    "            key=lambda x: int(re.findall(r'segment_(\\d+)', x)[0])\n",
    "        )\n",
    "\n",
    "    def process_faces_files(self, directory):\n",
    "        face_files = []\n",
    "        Faces_Folder =  os.path.join(directory, 'faces')\n",
    "        for filename in os.listdir(Faces_Folder):\n",
    "            match = re.match(self.pattern, filename)\n",
    "            if match:\n",
    "                frame_number = int(match.group(1))\n",
    "                face_number = int(match.group(2))\n",
    "                face_files.append({\n",
    "                    'filename': filename,\n",
    "                    'frame_number': frame_number,\n",
    "                    'face_number': face_number\n",
    "                })\n",
    "        face_files.sort(key=lambda x: (x['frame_number'], x['face_number']))\n",
    "        return face_files\n",
    "\n",
    "\n",
    "    def audio_labels(self, video_folder):\n",
    "        category = self.get_category_from_path(video_folder)\n",
    "        segment_files = self.get_sorted_npy_segment_files(video_folder)\n",
    "        \n",
    "        total_samples = sum(self.load_segment(os.path.join(video_folder, f)).shape[0] for f in segment_files)\n",
    "        print(f' damples for {total_samples}')\n",
    "        continuous_labels = self.create_continuous_labels(total_samples, category)\n",
    "        self.save_continuous_labels(continuous_labels, video_folder)\n",
    "\n",
    "        current_index = 0\n",
    "        for segment_file in segment_files:\n",
    "            segment_path = os.path.join(video_folder, segment_file)\n",
    "            segment_data = self.load_segment(segment_path)\n",
    "            segment_labels = continuous_labels[current_index:current_index + segment_data.shape[0]]\n",
    "\n",
    "            print(f\"Segment: {segment_file}, Shape: {segment_data.shape}, Labels: {segment_labels}\")\n",
    "            current_index += segment_data.shape[0]\n",
    "    \n",
    "    def visual_labels(self, video_folder):\n",
    "        category = self.get_category_from_path(video_folder)\n",
    "        segment_folders = self.get_sorted_visual_segment_folders(video_folder)\n",
    "\n",
    "        total_samples = 0\n",
    "        all_face_files = []\n",
    "\n",
    "        for segment_folder in segment_folders:\n",
    "            segment_path = os.path.join(video_folder, segment_folder)\n",
    "            all_face_files.extend(self.process_faces_files(segment_path))\n",
    "\n",
    "        total_samples = len(all_face_files)\n",
    "\n",
    "        continuous_labels = self.create_continuous_labels(total_samples, category)\n",
    "        continuous_labels_path = self.save_continuous_labels(continuous_labels, video_folder)\n",
    "        print(f\"Total faces in the video are : {total_samples}\")\n",
    "\n",
    "        current_index = 0\n",
    "        for segment_folder in segment_folders:\n",
    "            segment_path = os.path.join(video_folder, segment_folder)\n",
    "            segment_face_files = self.process_faces_files(segment_path)\n",
    "            \n",
    "            print(f\"{segment_folder} has {len(segment_face_files)} faces\")\n",
    "            print(\"Frame\\tFace\\tLabel\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "            for face_file in segment_face_files:\n",
    "                face_label = continuous_labels[current_index]\n",
    "                print(f\"{face_file['frame_number']}\\t{face_file['face_number']}\\t{face_label}\")\n",
    "                current_index += 1\n",
    "\n",
    "\n",
    "    def processs_2_modalities(self):\n",
    "        for category in [\"PTSD\", \"Non-PTSD\"]:\n",
    "            category_path = os.path.join(self.config['data_directory'], category)\n",
    "            for video_folder in os.listdir(category_path):\n",
    "                video_path = os.path.join(category_path, video_folder)\n",
    "                if os.path.isdir(video_path):\n",
    "                    print(f\"\\nProcessing video: {video_folder}\")\n",
    "                    if self.config['data_directory'].endswith('Audio'):\n",
    "                        self.audio_labels(video_path)\n",
    "                    elif self.config['data_directory'].endswith('Visual'):\n",
    "                        self.visual_labels(video_path)\n",
    "\n",
    "def main():\n",
    "    base_directory = '/media/scratch/datasets/PTSD_Project_train_validation_test_split/Final_train_new'\n",
    "    for modality in [ 'Audio', 'Visual']:\n",
    "        config = {'data_directory': os.path.join(base_directory, modality)}\n",
    "        print(f\"\\nProcessing {modality} data:\")\n",
    "        preprocessor = Create_Labels(config)\n",
    "        preprocessor.processs_2_modalities()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label loading and creating batches (in a sequential-manner) for Audio and Visual from Final_Train (2nd run)\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Audio Modality Classes and Functions\n",
    "class Load_Audio_Segments:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.video_folders = sorted([f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))])\n",
    "        self.segment_data = []  \n",
    "        self.video_boundaries = []  \n",
    "        \n",
    "        for video_folder in self.video_folders:\n",
    "            folder_path = os.path.join(root_dir, video_folder)\n",
    "            all_labels = torch.from_numpy(np.load(os.path.join(folder_path, f\"{video_folder}_Continuous_Label.npy\"))).float().to(device)\n",
    "            \n",
    "            segment_files = sorted(\n",
    "                [f for f in os.listdir(folder_path) if f.startswith('segment_') and f.endswith('.npy')],\n",
    "                key=lambda x: int(re.search(r'segment_(\\d+)\\.npy', x).group(1)))\n",
    "            \n",
    "            video_start = len(self.segment_data)\n",
    "            current_pos = 0  \n",
    "            \n",
    "            for seg_file in segment_files:\n",
    "                seg_path = os.path.join(folder_path, seg_file)\n",
    "                audio_tensor = torch.from_numpy(np.load(seg_path)).float().to(device)\n",
    "                num_frames = audio_tensor.shape[0]\n",
    "                \n",
    "                label_tensor = all_labels[current_pos:current_pos + num_frames]\n",
    "                current_pos += num_frames\n",
    "                \n",
    "                self.segment_data.append((audio_tensor, label_tensor, seg_file, seg_path))\n",
    "            self.video_boundaries.append((video_start, len(self.segment_data)))\n",
    "\n",
    "class VideoSegmentDataset(Dataset):\n",
    "    def __init__(self, segments):\n",
    "        self.segments = segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_tensor, label_tensor, _, _ = self.segments[idx]\n",
    "        return audio_tensor.to(device), label_tensor.to(device)\n",
    "\n",
    "def no_batch_collate_fn(batch):\n",
    "    return batch[0]\n",
    "\n",
    "def process_audio_modality(config):\n",
    "    processed_data = {'PTSD': {'Audio': {}}, 'Non-PTSD': {'Audio': {}}}\n",
    "    for category in [\"PTSD\", \"Non-PTSD\"]:\n",
    "        category_path = os.path.join(config['data_directory'], category)\n",
    "        if not os.path.exists(category_path):\n",
    "            continue\n",
    "        dataset = Load_Audio_Segments(category_path)\n",
    "        for vid_idx, (video_start, video_end) in enumerate(dataset.video_boundaries):\n",
    "            video_name = dataset.video_folders[vid_idx]\n",
    "            video_segments = dataset.segment_data[video_start:video_end]\n",
    "            video_dataset = VideoSegmentDataset(video_segments)\n",
    "            dataloader = DataLoader(\n",
    "                video_dataset,\n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                collate_fn=no_batch_collate_fn)\n",
    "            segments = []\n",
    "            for batch in dataloader:\n",
    "                segments.append(batch)\n",
    "            processed_data[category]['Audio'][video_name] = segments\n",
    "    return processed_data\n",
    "\n",
    "# Visual Modality Classes and Functions\n",
    "class Load_Visual_Labels_batches(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.video_folders = sorted([f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))])\n",
    "        self.segments = []\n",
    "        self.transform = transform\n",
    "        self.segment_info = []\n",
    "        self.label_files = {}\n",
    "        self.label_counts = {}\n",
    "        self.pattern = r'^frame_(\\d{6})_face_(\\d{2})\\.jpg$'\n",
    "\n",
    "        for video_folder in self.video_folders:\n",
    "            folder_path = os.path.join(root_dir, video_folder)\n",
    "            segment_folders = sorted([f for f in os.listdir(folder_path) if f.startswith('segment_')],\n",
    "                                     key=lambda x: int(re.search(r'segment_(\\d+)', x).group(1)))\n",
    "            labels_path = os.path.join(folder_path, f\"{video_folder}_Continuous_Label.npy\")\n",
    "            self.label_files[video_folder] = np.load(labels_path)\n",
    "            self.label_counts[video_folder] = len(self.label_files[video_folder])\n",
    "\n",
    "            current_start = 0\n",
    "            for segment_folder in segment_folders:\n",
    "                segment_path = os.path.join(folder_path, segment_folder, 'faces')\n",
    "                face_files = self.process_files(segment_path)\n",
    "                num_faces = len(face_files)\n",
    "                if num_faces == 0:\n",
    "                    print(f\" {segment_folder} skipping as it has no faces\")\n",
    "                    continue\n",
    "                self.segments.append((segment_path, face_files))\n",
    "                self.segment_info.append((video_folder, segment_folder, len(self.segments) - 1, num_faces, current_start))\n",
    "                current_start += num_faces\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segment_info)\n",
    "\n",
    "    def process_files(self, directory):\n",
    "        face_files = []\n",
    "        for filename in os.listdir(directory):\n",
    "            match = re.match(self.pattern, filename)\n",
    "            if match:\n",
    "                face_files.append(filename)\n",
    "        return sorted(face_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_folder, segment_folder, segment_idx, num_faces, label_start = self.segment_info[idx]\n",
    "        segment_path, face_files = self.segments[segment_idx]\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        for i, face_file in enumerate(face_files):\n",
    "            face_image_path = os.path.join(segment_path, face_file)\n",
    "            image = Image.open(face_image_path).convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            images.append(image)\n",
    "            label_index = label_start + i\n",
    "            if label_index < self.label_counts[video_folder]:\n",
    "                labels.append(self.label_files[video_folder][label_index])\n",
    "            else:\n",
    "                labels.append(self.label_files[video_folder][-1])           \n",
    "        return torch.stack(images), torch.tensor(labels).float()\n",
    "\n",
    "\n",
    "def process_visual_modality(config):\n",
    "    processed_data = {'PTSD': {'Visual': {}}, 'Non-PTSD': {'Visual': {}}}\n",
    "    for category in [\"PTSD\", \"Non-PTSD\"]:\n",
    "        category_path = os.path.join(config['data_directory'], category)\n",
    "        if not os.path.exists(category_path):\n",
    "            continue\n",
    "        dataset = Load_Visual_Labels_batches(category_path, transform=config['transform'])\n",
    "        for video_folder in dataset.video_folders:\n",
    "            video_segments = [seg for seg in dataset.segment_info if seg[0] == video_folder]\n",
    "            segments = []\n",
    "            for segment_info in video_segments:\n",
    "                _, segment_folder, segment_idx, _, _ = segment_info\n",
    "                _, face_files = dataset.segments[segment_idx]\n",
    "                images, labels = dataset[segment_idx]\n",
    "                #Display_images_with_labels(images, labels, face_files, segment_folder)\n",
    "                segments.append((images, labels))\n",
    "            \n",
    "            processed_data[category]['Visual'][video_folder] = segments\n",
    "    return processed_data\n",
    "\n",
    "# Main Processing\n",
    "base_directory = '/media/scratch/datasets/PTSD_Project_train_validation_test_split'\n",
    "dataset_types = ['Final_train_new']\n",
    "final_datasets = {dataset_type: {'PTSD': {'Audio': {}, 'Visual': {}}, 'Non-PTSD': {'Audio': {}, 'Visual': {}}} for dataset_type in dataset_types}\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    dataset_dir = os.path.join(base_directory, dataset_type)\n",
    "    \n",
    "    # Process Audio\n",
    "    audio_config = {'data_directory': os.path.join(dataset_dir, 'Audio')}\n",
    "    audio_data = process_audio_modality(audio_config)\n",
    "    \n",
    "    # Process Visual\n",
    "    visual_config = {\n",
    "        'data_directory': os.path.join(dataset_dir, 'Visual'),\n",
    "        'transform': transforms.Compose([transforms.Resize((48, 48)), transforms.ToTensor()])\n",
    "    }\n",
    "    visual_data = process_visual_modality(visual_config)\n",
    "    \n",
    "    # Merge data\n",
    "    for category in ['PTSD', 'Non-PTSD']:\n",
    "        if category in audio_data and 'Audio' in audio_data[category]:\n",
    "            for video, segments in audio_data[category]['Audio'].items():\n",
    "                final_datasets[dataset_type][category]['Audio'][video] = segments\n",
    "        if category in visual_data and 'Visual' in visual_data[category]:\n",
    "            for video, segments in visual_data[category]['Visual'].items():\n",
    "                final_datasets[dataset_type][category]['Visual'][video] = segments\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    for category in ['PTSD', 'Non-PTSD']:\n",
    "        for modality in ['Audio', 'Visual']:\n",
    "            for video in final_datasets[dataset_type][category][modality]:\n",
    "                print(modality, video)\n",
    "                segments = final_datasets[dataset_type][category][modality][video]\n",
    "                for i in range(len(segments)):\n",
    "                    print(i)\n",
    "                    samples, labels = segments[i]\n",
    "                    print('samples is ', (samples.shape))\n",
    "                    labels_np = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "                    length = labels_np.shape[0]\n",
    "                    print('length is ', length)\n",
    "                    pad_value = 1.0 if category == \"PTSD\" else 0.0\n",
    "                    if length < 500:\n",
    "                        padded_labels = np.pad(labels_np, (0, 500 - length), mode='constant', constant_values=pad_value)\n",
    "                    else:\n",
    "                        padded_labels = labels_np[:500]\n",
    "                    padded_labels = torch.tensor(padded_labels, dtype=torch.float32).to(device)\n",
    "                    segments[i] = (samples.to(device), padded_labels)\n",
    "            print(\"--------\")\n",
    "X_processed = {'video': {'PTSD': final_datasets['Final_train_new']['PTSD']['Visual'],'Non-PTSD': final_datasets['Final_train_new']['Non-PTSD']['Visual']},\n",
    "    'logmel': {'PTSD': final_datasets['Final_train_new']['PTSD']['Audio'], 'Non-PTSD': final_datasets['Final_train_new']['Non-PTSD']['Audio']}}\n",
    "for modality in ['video', 'logmel']:\n",
    "    for category in ['PTSD', 'Non-PTSD']:\n",
    "        print(f\"\\n {modality} {category}\")\n",
    "        video_dict = X_processed[modality][category]\n",
    "        \n",
    "        for video_name, segments in video_dict.items():\n",
    "            print(f\"\\n  Video: {video_name}\")\n",
    "            print(f\"  Number of segments: {len(segments)}\")\n",
    "            '''for seg_idx, (samples, labels) in enumerate(segments):\n",
    "                print(f\"\\n    Segment {seg_idx + 1}:\")\n",
    "                print(f\"      Samples shape: {samples.shape}\")  \n",
    "                print(f\"      Labels shape: {labels.shape}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import Linear, BatchNorm1d, BatchNorm2d, Dropout, Sequential, Module\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "regressor1 = nn.Linear(160, 256).to(device)\n",
    "bn1 = BatchNorm1d(256).to(device)\n",
    "regressor2 = nn.Linear(256, 1).to(device)\n",
    "\n",
    "class DenseCoAttn(nn.Module):\n",
    "\tdef __init__(self, dim1, dim2, dropout):\n",
    "\t\tsuper(DenseCoAttn, self).__init__()\n",
    "\t\tdim = dim1 + dim2\n",
    "\t\tself.dropouts = nn.ModuleList([nn.Dropout(dropout) for _ in range(2)])\n",
    "\t\tself.query_linear = nn.Linear(dim, dim)\n",
    "\n",
    "\t\tself.key1_linear = nn.Linear(500, 500)\n",
    "\t\tself.key2_linear = nn.Linear(500, 500)\n",
    "\t\tself.value1_linear = nn.Linear(dim1, dim1)\n",
    "\t\tself.value2_linear = nn.Linear(dim2, dim2)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, value1, value2):\n",
    "\n",
    "\t\tjoint = torch.cat((value1, value2), dim=-1)\n",
    "\t\t# audio  audio*W*joint\n",
    "\t\tva_joint = self.query_linear(joint)\n",
    "\t\tkey1 = self.key1_linear(value1.transpose(1, 2))\n",
    "\t\tkey2 = self.key2_linear(value2.transpose(1, 2))\n",
    "\n",
    "\t\tvalue1 = self.value1_linear(value1)\n",
    "\t\tvalue2 = self.value2_linear(value2)\n",
    "          \n",
    "\t\tweighted1, attn1 = self.qkv_attention(joint, key1, value1, dropout=self.dropouts[0])\n",
    "\t\tweighted2, attn2 = self.qkv_attention(joint, key2, value2, dropout=self.dropouts[1])\n",
    "          \n",
    "\t\treturn weighted1, weighted2\n",
    "     \n",
    "\tdef qkv_attention(self, query, key, value, dropout=None):\n",
    "\t\td_k = query.size(-1)\n",
    "\t\tscores = torch.bmm(key, query) / math.sqrt(d_k)\n",
    "\t\tscores = torch.tanh(scores)\n",
    "        \n",
    "\t\tif dropout:\n",
    "\t\t\tscores = dropout(scores)\n",
    "\t\tweighted = torch.tanh(torch.bmm(value, scores))\n",
    "\t\treturn self.relu(weighted), scores\n",
    "\n",
    "class NormalSubLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim1, dim2, dropout):\n",
    "        super(NormalSubLayer, self).__init__()\n",
    "        self.dense_coattn = DenseCoAttn(dim1, dim2, dropout)\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1 + dim2, dim2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout))])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        weighted1, weighted2 = self.dense_coattn(data1, data2)\n",
    "        data1 = data1 + self.linears[0](weighted1)\n",
    "        data2 = data2 + self.linears[1](weighted2)\n",
    "        return data1, data2\n",
    "\n",
    "class DCNLayer(nn.Module):\n",
    "    def __init__(self, dim1, dim2, num_seq, dropout):\n",
    "        super(DCNLayer, self).__init__()\n",
    "        self.dcn_layers = nn.ModuleList([NormalSubLayer(dim1, dim2, dropout) for _ in range(num_seq)])\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        for dense_coattn in self.dcn_layers:\n",
    "            data1, data2 = dense_coattn(data1, data2)\n",
    "        return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:43:21.240793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744400601.248390 1446635 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744400601.250698 1446635 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744400601.257747 1446635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744400601.257752 1446635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744400601.257753 1446635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744400601.257755 1446635 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-11 15:43:21.259889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torch\n",
    "from base.vggish.vggish import VGGish\n",
    "from models.temporal_convolutional_model import TemporalConvNet\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import Linear, BatchNorm1d, BatchNorm2d, Dropout, Sequential, Module\n",
    "import torch.nn.functional as F\n",
    "from models.arcface_model import Backbone\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "#working with the audio\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 6, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 128))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.transpose(x, 1, 3)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = x.contiguous()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embeddings(x)\n",
    "        return x\n",
    "\n",
    "def make_layers():\n",
    "    layers = []\n",
    "    in_channels = 1  \n",
    "    for v in [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\"]:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _vgg():\n",
    "    return VGG(make_layers())\n",
    "\n",
    "class VGGish(VGG):\n",
    "    def __init__(self):\n",
    "        super().__init__(make_layers())\n",
    "\n",
    "    def forward(self, x, fs=None):\n",
    "        x = VGG.forward(self, x)\n",
    "        return x\n",
    "    \n",
    "class AudioBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = VGGish()\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "#working with the visual\n",
    "class VisualBackbone(nn.Module):\n",
    "    def __init__(self, input_channels=3, num_classes=8, use_pretrained=True, state_dict_path=\"/media/scratch/Trauma_Detection/code/RestNet50.pth\", mode=\"ir\",\n",
    "                 embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone(input_channels=input_channels, num_layers=50, drop_ratio=0.4, mode=mode)\n",
    "        if use_pretrained:\n",
    "            state_dict = torch.load(state_dict_path, map_location=device)\n",
    "\n",
    "            if \"backbone\" in list(state_dict.keys())[0]:\n",
    "\n",
    "                self.backbone.output_layer = Sequential(BatchNorm2d(embedding_dim),\n",
    "                                                        Dropout(0.4),\n",
    "                                                        Flatten(),\n",
    "                                                        Linear(embedding_dim * 5 * 5, embedding_dim),\n",
    "                                                        BatchNorm1d(embedding_dim))\n",
    "                new_state_dict = {}\n",
    "                for key, value in state_dict.items():\n",
    "\n",
    "                    if \"logits\" not in key:\n",
    "                        new_key = key[9:]\n",
    "                        new_state_dict[new_key] = value\n",
    "\n",
    "                self.backbone.load_state_dict(new_state_dict)\n",
    "            else:\n",
    "                self.backbone.load_state_dict(state_dict)\n",
    "\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.backbone.output_layer = Sequential(BatchNorm2d(embedding_dim),\n",
    "                                                Dropout(0.4),\n",
    "                                                Flatten(),\n",
    "                                                Linear(embedding_dim * 5 * 5, embedding_dim),\n",
    "                                                BatchNorm1d(embedding_dim))\n",
    "        \n",
    "        self.backbone.output_layer = Sequential(\n",
    "        BatchNorm2d(embedding_dim),\n",
    "        Dropout(0.4),\n",
    "        nn.AdaptiveAvgPool2d((5, 5)),\n",
    "        Flatten(),\n",
    "        Linear(embedding_dim * 5 * 5, embedding_dim),\n",
    "        BatchNorm1d(embedding_dim))\n",
    "\n",
    "        self.logits = nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "\n",
    "        from torch.nn.init import xavier_uniform_, constant_\n",
    "\n",
    "        for m in self.backbone.output_layer.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight = xavier_uniform_(m.weight)\n",
    "                m.bias = constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "        self.logits.weight = xavier_uniform_(self.logits.weight)\n",
    "        self.logits.bias = constant_(self.logits.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def extract(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class Joint_Module(nn.Module):\n",
    "    def __init__(self, modality=['video', 'logmel'], kernel_size=5, example_length=500, tcn_attention=0,\n",
    "                 tcn_channel={'video': [512, 256, 256, 128], 'cnn_res50': [512, 256, 256, 128], 'mfcc':[32, 32, 32, 32], 'vggish': [32, 32, 32, 32], 'logmel': [32, 32, 32, 32]},\n",
    "                 embedding_dim={'video': 512,  'bert': 768, 'cnn_res50': 512, 'mfcc': 39, 'vggish': 128, 'logmel': 128, 'egemaps': 88},\n",
    "                 encoder_dim={'video': 128, 'bert': 128, 'cnn_res50': 128, 'mfcc': 32, 'vggish': 32, 'logmel': 32, 'egemaps': 32},\n",
    "                 modal_dim=32, num_heads=2,\n",
    "                 root_dir='', device=device):\n",
    "        super().__init__()\n",
    "        self.examples_segment = segments\n",
    "        self.root_dir = root_dir\n",
    "        self.device = device\n",
    "        self.modality = modality\n",
    "        self.kernel_size = kernel_size\n",
    "        self.example_length = example_length\n",
    "        self.tcn_channel = tcn_channel\n",
    "        self.tcn_attention = tcn_attention\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.outputs = {}\n",
    "        self.temporal, self.fusion = nn.ModuleDict(), None\n",
    "        self.num_heads = num_heads\n",
    "        self.modal_dim = modal_dim\n",
    "        self.final_dim = self.embedding_dim[self.modality[0]] + self.embedding_dim[self.modality[1]] + self.embedding_dim[self.modality[0]]\n",
    "        self.spatial = nn.ModuleDict()\n",
    "        self.bn = nn.ModuleDict()\n",
    "        self.Length = 500\n",
    "        self.spatial_visual_model = VisualBackbone().to(device)\n",
    "        self.spatial_audio_model = AudioBackbone().to(device)\n",
    "\n",
    "    \n",
    "    def init(self):\n",
    "        self.output_dim = 1\n",
    "        \n",
    "        for modality in self.modality:\n",
    "            self.temporal[modality] = TemporalConvNet(num_inputs=self.embedding_dim[modality], max_length=self.example_length,\n",
    "                                                   num_channels=self.tcn_channel[modality], attention=self.tcn_attention,\n",
    "                                                   kernel_size=self.kernel_size, dropout=0.1).to(self.device)\n",
    "            self.bn[modality] = BatchNorm1d(self.tcn_channel[modality][-1])\n",
    "\n",
    "        self.coattn = DCNLayer(128, 32, 2, 0.6)\n",
    "        #self.regressor1 = nn.Linear(512, 256) --Initial\n",
    "        self.regressor1 = nn.Linear(256, 256)\n",
    "        self.bn1 = BatchNorm1d(256)\n",
    "        self.regressor2 = nn.Linear(256, self.output_dim)\n",
    "\n",
    "    def forward(self,X):\n",
    "        Video, Audio = self.coattn(X['visual_features'][0],X['logmel_features'][0])\n",
    "        c = torch.cat((Video, Audio), dim=-1)\n",
    "        c = regressor1(c).transpose(1, 2)\n",
    "        c = bn1(c).transpose(1, 2)\n",
    "        c = F.leaky_relu(c)\n",
    "        c = regressor2(c)\n",
    "        c = torch.tanh(c)\n",
    "        return c\n",
    "\n",
    "    def forward_video(self, segment):\n",
    "        batch_input = torch.tensor(segment, dtype=torch.float32).to(self.device)\n",
    "        batch_input = batch_input.unsqueeze(0)\n",
    "        batch_size, length, channel, width, height = batch_input.shape\n",
    "        batch_input = batch_input.view(-1, channel, width, height)\n",
    "        batch_input = batch_input.to(device) \n",
    "        batch_input = self.spatial_visual_model(batch_input)\n",
    "        batch_input = batch_input.to(self.device)\n",
    "        _, feature_dim = batch_input.shape\n",
    "        video_features = batch_input.view(batch_size, length, feature_dim).unsqueeze(1)\n",
    "        return video_features.squeeze(1).transpose(1, 2)  \n",
    "\n",
    "    def forward_logmel(self, segment):  \n",
    "        segment = torch.tensor(segment, dtype=torch.float32).to(self.device)\n",
    "        batch_input = segment.unsqueeze(1).float().to(self.device)\n",
    "        batch_size = batch_input.shape[1]\n",
    "        length = batch_input.shape[0]\n",
    "        vggish_features = self.spatial_audio_model(batch_input)\n",
    "        #vggish_features = vggish_features.cpu().detach().numpy()\n",
    "        _, feature_dim = vggish_features.shape\n",
    "        logmel_features = torch.tensor(vggish_features).view(batch_size, length, feature_dim).unsqueeze(1)\n",
    "        return logmel_features.squeeze(1).transpose(1, 2)\n",
    "\n",
    "    def compact_feature(self, spatial_tensor):\n",
    "        sequence_length = spatial_tensor.shape[2]\n",
    "        length_difference = self.Length - sequence_length\n",
    "        if length_difference > 0:\n",
    "            last_slice = spatial_tensor[:, :, -1:]\n",
    "            padding = last_slice.repeat(1, 1, length_difference)\n",
    "            tensor = torch.cat((spatial_tensor, padding), dim=2)\n",
    "        else:\n",
    "            tensor = spatial_tensor\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "class GenericParamControl(object):\n",
    "    @staticmethod\n",
    "    def init_module_list():\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def init_param_group():\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_param_group(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def release_param(self, model):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ResnetParamControl(GenericParamControl):\n",
    "    def __init__(self, trainer, gradual_release=1, release_count=8, backbone_mode=\"ir\"):\n",
    "        self.trainer = trainer\n",
    "        self.gradual_release = gradual_release\n",
    "        self.release_count = release_count\n",
    "        self.backbone_mode = backbone_mode\n",
    "        self.module_dict = self.init_module_list()\n",
    "        self.module_stack = self.init_param_group()\n",
    "        self.early_stop = False\n",
    "\n",
    "    def init_module_list(self):\n",
    "        return {\n",
    "            \"spatial_visual_model\": [\n",
    "                [(4, 10)], \n",
    "                [(163, 187)], \n",
    "                [(142, 163)]\n",
    "            ],\n",
    "            \"spatial_audio_model\": [\n",
    "                [(16, 18)], \n",
    "                [(14, 16)], \n",
    "                [(12, 14)]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def init_param_group(self):\n",
    "        module_stack = {\"spatial_visual_model\": [], \"spatial_audio_model\": []}\n",
    "        for modal, ranges in self.module_dict.items():\n",
    "            for groups in ranges:\n",
    "                slice_range = []\n",
    "                for group in groups:\n",
    "                    slice_range += list(np.arange(*group))\n",
    "                module_stack[modal].append(slice_range)\n",
    "        return module_stack\n",
    "\n",
    "    def get_param_group(self, modal):\n",
    "        return self.module_stack[modal].pop(0)\n",
    "\n",
    "    def get_current_lr(self):\n",
    "        return self.trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    def release_param(self, model, epoch=0, modalities=['spatial_visual_model', 'spatial_audio_model']):\n",
    "        if self.gradual_release and self.release_count > 0:\n",
    "            for modal in modalities:\n",
    "                indices = self.get_param_group(modal) \n",
    "                print(f\"Releasing parameters for: {modal}\")\n",
    "                print(f\"Releasing layers in range: {list(indices)}\")\n",
    "                modal_submodel = getattr(model, modal)  \n",
    "                modal_params = list(modal_submodel.parameters())\n",
    "                for param in itemgetter(*indices)(modal_params):\n",
    "                    param.requires_grad = True\n",
    "            self.trainer.init_optimizer_and_scheduler(epoch=epoch)\n",
    "            self.release_count -= 1\n",
    "            self.trainer.early_stopping_counter = self.trainer.early_stopping\n",
    "        else:\n",
    "            print(\"No more parameters to release!\")\n",
    "            self.early_stop = True\n",
    "\n",
    "    def load_trainer(self, trainer):\n",
    "        self.trainer = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagasaithadishetty/miniconda3/envs/dinov2/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/tmp/ipykernel_1446635/1244715274.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_input = torch.tensor(segment, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video\n",
      "-----\n",
      "vidm031 with 33 segments of PTSD\n",
      "Great_Day with 35 segments of Non-PTSD\n",
      "logmel\n",
      "-----\n",
      "vidm031 with 33 segments of PTSD\n",
      "Great_Day with 35 segments of Non-PTSD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1446635/1244715274.py:217: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment = torch.tensor(segment, dtype=torch.float32).to(self.device)\n",
      "/tmp/ipykernel_1446635/1244715274.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  logmel_features = torch.tensor(vggish_features).view(batch_size, length, feature_dim).unsqueeze(1)\n"
     ]
    }
   ],
   "source": [
    "model = Joint_Module()\n",
    "model.init()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "Spatial_visual_features, Temporal_visual_features = {}, {}\n",
    "Spatial_logmel_features, Temporal_logmel_features = {}, {}\n",
    "\n",
    "for modality, data in X_processed.items():\n",
    "    print(f\"{modality}\\n-----\")\n",
    "    process_fn = model.forward_logmel if modality == 'logmel' else model.forward_video  \n",
    "\n",
    "    for category, videos in data.items():\n",
    "        if modality == 'logmel':\n",
    "            Spatial_logmel_features.setdefault(category, {})\n",
    "            Temporal_logmel_features.setdefault(category, {})\n",
    "        else:\n",
    "            Spatial_visual_features.setdefault( category, {})\n",
    "            Temporal_visual_features.setdefault( category, {})\n",
    "        \n",
    "        for video_name, segments in videos.items():\n",
    "            print(f\"{video_name} with {len(segments)} segments of {category}\")\n",
    "\n",
    "            spatial_dict = Spatial_logmel_features if modality == 'logmel' else Spatial_visual_features\n",
    "            temporal_dict = Temporal_logmel_features if modality == 'logmel' else Temporal_visual_features\n",
    "            \n",
    "            spatial_dict[category].setdefault(video_name, [])\n",
    "            temporal_dict[category].setdefault(video_name, [])\n",
    "            \n",
    "            for i, (tensor, labels) in enumerate(segments[:3]):\n",
    "                if i >= 3: \n",
    "                    break\n",
    "                tensor = tensor.to(device)\n",
    "                output = process_fn(tensor)\n",
    "                spatial_features = output if isinstance(output, torch.Tensor) else output[0]\n",
    "                spatial_features = model.compact_feature(spatial_features).to(device)\n",
    "                spatial_dict[category][video_name].append([spatial_features, labels])\n",
    "                temporal_features= model.bn[modality](model.temporal[modality](spatial_features.to(device))).transpose(1, 2).to(device)\n",
    "                temporal_dict[category][video_name].append([temporal_features,labels])\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 473.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vidm031'] ['PTSD'] 3\n",
      "torch.Size([3, 500, 1])\n",
      "['Great_Day'] ['Non-PTSD'] 3\n",
      "torch.Size([3, 500, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "Y = {'logmel': {category: {video_name: Temporal_logmel_features[category][video_name] for video_name in Temporal_logmel_features[category]} for category in Temporal_logmel_features},\n",
    "    'video': {category: {video_name: Temporal_visual_features[category][video_name] for video_name in Temporal_visual_features[category]} for category in Temporal_visual_features}}\n",
    "\n",
    "def stacking_video_wise(temporal_dict):\n",
    "    video_wise_data = [] \n",
    "    for category in temporal_dict:\n",
    "        for video_name in temporal_dict[category]:\n",
    "            features_list = []\n",
    "            for segment in temporal_dict[category][video_name]:\n",
    "                features_list.append(segment[0])\n",
    "            if features_list:\n",
    "                video_wise_data.append({\n",
    "                    'video_name': video_name,\n",
    "                    'category': category,\n",
    "                    'features': torch.stack(features_list,dim=0)})\n",
    "    return video_wise_data\n",
    "\n",
    "temporal_visual_features = stacking_video_wise(Y['video'])\n",
    "temporal_logmel_features = stacking_video_wise(Y['logmel'])\n",
    "\n",
    "for visual_item, logmel_item in zip(temporal_visual_features, temporal_logmel_features):\n",
    "    visual_item['features'] = visual_item['features'].squeeze(1)\n",
    "    logmel_item['features'] = logmel_item['features'].squeeze(1)\n",
    "\n",
    "def align_features(visual_data, logmel_data):\n",
    "    aligned_data = []\n",
    "    for vis_item, log_item in zip(visual_data, logmel_data):\n",
    "        assert vis_item['video_name'] == log_item['video_name']\n",
    "        num_segments = vis_item['features'].shape[0]\n",
    "        print(num_segments)\n",
    "        aligned_data.append({\n",
    "            'video_name': vis_item['video_name'],\n",
    "            'category': vis_item['category'],\n",
    "            'visual_features': vis_item['features'],\n",
    "            'logmel_features': log_item['features'],\n",
    "            'label':  torch.full((num_segments, 500, 1), \n",
    "                                1 if vis_item['category'] == 'PTSD' else 0,\n",
    "                                dtype=torch.float32),\n",
    "            'segments': num_segments} )\n",
    "    return aligned_data\n",
    "\n",
    "aligned_data = align_features(temporal_visual_features, temporal_logmel_features)\n",
    "def init_dataloader(data, batch_size=1, shuffle=False):\n",
    "    dataloaders = DataLoader(data,batch_size=1,shuffle=shuffle)\n",
    "    return dataloaders\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_dataloader = init_dataloader(aligned_data, batch_size=1, shuffle=False)\n",
    "dataloader_dict = {'train': train_dataloader,  'validate': train_dataloader }\n",
    "num_batch_warm_up = len(train_dataloader)\n",
    "for batch_idx, batch in tqdm(enumerate(dataloader_dict['train']), total=len(dataloader_dict['train'])):\n",
    "    batch = {key: value.to(device) if isinstance(value, torch.Tensor) else value for key, value in batch.items()}\n",
    "    print(batch['video_name'], batch['category'], batch['label'].shape[1])\n",
    "    with torch.no_grad():\n",
    "        output = model(batch)\n",
    "        print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RJCMA.base.scheduler import  MyWarmupScheduler\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def init_optimizer_and_scheduler(self, epoch=0):\n",
    "        self.optimizer = optim.Adam(self.get_parameters(), lr=self.learning_rate, weight_decay=0.001)\n",
    "        self.scheduler = MyWarmupScheduler(\n",
    "            optimizer=self.optimizer, lr = self.learning_rate, min_lr=self.min_learning_rate,\n",
    "            best=self.best_epoch_info['ccc'], mode=\"max\", patience=self.patience,\n",
    "            factor=self.factor, num_warmup_epoch=self.min_epoch, init_epoch=epoch)\n",
    "            \n",
    "    def Train(self, dataloader_dict, checkpoint_controller, parameter_controller):\n",
    "        print(\"------\\nStarting training on device:\", self.device)\n",
    "        self.time_fit_start = time.time()\n",
    "        if self.best_epoch_info is None:\n",
    "                self.best_epoch_info = {'model_weights': copy.deepcopy(self.model.state_dict()),'loss': 1e10,'ccc': -1e10}\n",
    "        for epoch in range(self.start_epoch, self.max_epoch):\n",
    "            if self.fit_finished:\n",
    "                print(\"\\nEarly Stop!\\n\")\n",
    "                break\n",
    "            \n",
    "            if (epoch in self.milestone or (parameter_controller.get_current_lr() < self.min_learning_rate and epoch >= self.min_epoch and self.scheduler.relative_epoch > self.min_epoch)):\n",
    "                parameter_controller.release_param(self.model.spatial, epoch)\n",
    "                if parameter_controller.early_stop:\n",
    "                    break\n",
    "                self.model.load_state_dict(self.best_epoch_info['model_weights'])\n",
    "            \n",
    "            time_epoch_start = time.time()\n",
    "            if self.verbose:\n",
    "                print(\"There are {} layers to update.\".format(len(self.optimizer.param_groups[0]['params'])))\n",
    "\n",
    "            train_loss = self.train(dataloader_dict['train'], epoch =epoch)\n",
    "            val_loss = self.validate(dataloader_dict['validate'], epoch=epoch)\n",
    "            \n",
    "            improvement = False\n",
    "            if val_loss > self.best_epoch_info['loss']:\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.save_path, \"model_state_dict\"  + \".pth\"))\n",
    "                improvement = True\n",
    "                self.best_epoch_info = {'model_weights': copy.deepcopy(self.model.state_dict()),'loss': val_loss,'epoch': epoch}\n",
    "\n",
    "            if self.verbose:\n",
    "                    print(\n",
    "                        \"\\n Fold {:2} Epoch {:2} in {:.0f}s || Train loss={:.3f} | Val loss={:.3f} | LR={:.1e} | Release_count={} | best={} | \"\n",
    "                        \"improvement={}-{}\".format(\n",
    "                            self.fold,\n",
    "                            epoch + 1,\n",
    "                            time.time() - time_epoch_start,\n",
    "                            train_loss,\n",
    "                            val_loss,\n",
    "                            self.optimizer.param_groups[0]['lr'],\n",
    "                            parameter_controller.release_count,\n",
    "                            int(self.best_epoch_info['epoch']) + 1,\n",
    "                            improvement,\n",
    "                            self.early_stopping_counter))\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if improvement:\n",
    "                    self.early_stopping_counter = self.early_stopping\n",
    "                else:\n",
    "                    self.early_stopping_counter -= 1\n",
    "\n",
    "                if self.early_stopping_counter <= 0 and epoch >= self.min_epoch:\n",
    "                    self.fit_finished = True\n",
    "\n",
    "            self.scheduler.step(metrics=val_loss, epoch=epoch)\n",
    "            self.start_epoch = epoch + 1\n",
    "\n",
    "            if self.load_best_at_each_epoch:\n",
    "                    self.model.load_state_dict(self.best_epoch_info['model_weights'])\n",
    "            checkpoint_controller.save_checkpoint(self, parameter_controller, self.save_path)\n",
    "        self.fit_finished = True\n",
    "        checkpoint_controller.save_checkpoint(self, parameter_controller, self.save_path)\n",
    "        self.model.load_state_dict(self.best_epoch_info['model_weights'])\n",
    "\n",
    "    def train(self, **kwargs):\n",
    "        self.model.train()\n",
    "        train_loss = self.fit(**kwargs)\n",
    "        return train_loss\n",
    "\n",
    "    def validate(self, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            val_loss = self.fit(**kwargs)\n",
    "        return val_loss\n",
    "\n",
    "    def fit(self, dataloader):\n",
    "        running_loss = 0.0\n",
    "        total_segments = 0  \n",
    "        num_batch_warm_up = len(dataloader) * self.min_epoch\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            self.scheduler.warmup_lr(self.learning_rate, batch_idx, num_batch_warm_up)\n",
    "            segment_count = len(batch['segments'])\n",
    "            total_segments += segment_count\n",
    "            labels = batch.pop('label')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(batch)\n",
    "            loss = self.criterion(labels, outputs)\n",
    "            running_loss += loss.mean().item() * segment_count \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / total_segments \n",
    "        return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RJCMA.base.loss_function import CCCLoss\n",
    "from RJCMA.base.checkpointer import Checkpointer\n",
    "import os\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        self.args = args\n",
    "        self.release_count = args.release_count\n",
    "        self.gradual_release = args.gradual_release\n",
    "        self.milestone = args.milestone\n",
    "        self.backbone_mode = \"ir\"\n",
    "        self.min_num_epochs = args.min_num_epochs\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.early_stopping = args.early_stopping\n",
    "        self.load_best_at_each_epoch = args.load_best_at_each_epoch\n",
    "\n",
    "        self.num_heads = args.num_heads\n",
    "        self.modal_dim = args.modal_dim\n",
    "        self.tcn_kernel_size = args.tcn_kernel_size\n",
    "\n",
    "    def run(self):\n",
    "        criterion = CCCLoss()\n",
    "        for fold in iter(self.folds_to_run):\n",
    "\n",
    "            save_path = os.path.join(self.save_path,\n",
    "                                     self.experiment_name + \"_\" + self.model_name + \"_\" + self.stamp + \"_fold\" + str(\n",
    "                                         fold) + \"_\" + self.emotion +  \"_seed\" + str(self.seed))\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "            checkpoint_filename = os.path.join(save_path, \"checkpoint.pkl\")\n",
    "\n",
    "            trainer_kwards = {'device': self.device, 'emotion': self.emotion, 'model_name': self.model_name,\n",
    "                              'models': model, 'save_path': save_path, 'fold': fold,\n",
    "                              'min_epoch': self.min_num_epochs, 'max_epoch': self.num_epochs,\n",
    "                              'early_stopping': self.early_stopping, 'scheduler': self.scheduler,\n",
    "                              'learning_rate': self.learning_rate, 'min_learning_rate': self.min_learning_rate,\n",
    "                              'patience': self.patience, 'batch_size': self.batch_size,\n",
    "                              'criterion': criterion, 'factor': self.factor, 'verbose': True,\n",
    "                              'milestone': self.milestone, 'metrics': self.config['metrics'],\n",
    "                              'load_best_at_each_epoch': self.load_best_at_each_epoch}\n",
    "\n",
    "            trainer = Trainer(**trainer_kwards)\n",
    "\n",
    "            parameter_controller = ResnetParamControl(trainer, gradual_release=self.gradual_release,\n",
    "                                                      release_count=self.release_count,\n",
    "                                                      backbone_mode=[\"spatial_visual_model\", \"spatial_audio_model\"])\n",
    "\n",
    "            checkpoint_controller = Checkpointer(checkpoint_filename, trainer, parameter_controller, resume=self.resume)\n",
    "\n",
    "            if self.resume:\n",
    "                trainer, parameter_controller = checkpoint_controller.load_checkpoint()\n",
    "\n",
    "            if not trainer.fit_finished:\n",
    "                trainer.train(dataloader_dict, parameter_controller=parameter_controller,\n",
    "                            checkpoint_controller=checkpoint_controller)\n",
    "\n",
    "    def get_selected_continuous_label_dim(self):\n",
    "        if self.emotion == \"PTSD\":\n",
    "            dim = [1]\n",
    "        elif self.emotion == \"Non-PTSD\":\n",
    "            dim = [0]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown emotion!\")\n",
    "        return dim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
